{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Pedestrian Classification with Normalized HOG Features\n",
    "\n",
    "This notebook implements pedestrian classification using normalized HOG (Histogram of Oriented Gradients) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.feature import hog\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "os.makedirs(\"Models\", exist_ok=True)\n",
    "os.makedirs(\"Results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pos(folder):\n",
    "    \"\"\"Load positive samples (pedestrians)\"\"\"\n",
    "    imgs = []\n",
    "    for f in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, f))\n",
    "        if img is not None:\n",
    "            imgs.append(cv2.resize(img, (64, 128)))\n",
    "    return imgs\n",
    "\n",
    "def load_neg(folder, n=5):\n",
    "    \"\"\"Load negative samples (non-pedestrians)\"\"\"\n",
    "    imgs = []\n",
    "    for f in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, f))\n",
    "        if img is None: \n",
    "            continue\n",
    "        h, w, _ = img.shape\n",
    "        for _ in range(n):\n",
    "            x = np.random.randint(0, w-64)\n",
    "            y = np.random.randint(0, h-128)\n",
    "            imgs.append(img[y:y+128, x:x+64])\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_pos = load_pos(\"../INRIAPerson/Train/pos\")\n",
    "train_neg = load_neg(\"../INRIAPerson/Train/neg\")\n",
    "\n",
    "val_pos = load_pos(\"../INRIAPerson/Validation/pos\")\n",
    "val_neg = load_neg(\"../INRIAPerson/Validation/neg\")\n",
    "\n",
    "X_train_imgs = train_pos + train_neg\n",
    "y_train = np.array([1]*len(train_pos) + [0]*len(train_neg))\n",
    "\n",
    "X_val_imgs = val_pos + val_neg\n",
    "y_val = np.array([1]*len(val_pos) + [0]*len(val_neg))\n",
    "\n",
    "print(f\"Train samples: {len(X_train_imgs)}\")\n",
    "print(f\"Validation samples: {len(X_val_imgs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "\n",
    "# Show positive samples\n",
    "for i in range(2):\n",
    "    img = cv2.cvtColor(train_pos[i], cv2.COLOR_BGR2RGB)\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(f'Positive {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Show negative samples\n",
    "for i in range(2):\n",
    "    img = cv2.cvtColor(train_neg[i], cv2.COLOR_BGR2RGB)\n",
    "    axes[1, i].imshow(img)\n",
    "    axes[1, i].set_title(f'Negative {i+1}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "# Show HOG visualizations\n",
    "for i in range(2):\n",
    "    gray_pos = cv2.cvtColor(train_pos[i], cv2.COLOR_BGR2GRAY)\n",
    "    gray_neg = cv2.cvtColor(train_neg[i], cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    _, hog_pos = hog(gray_pos, orientations=9, pixels_per_cell=(8, 8),\n",
    "                     cells_per_block=(2, 2), visualize=True)\n",
    "    _, hog_neg = hog(gray_neg, orientations=9, pixels_per_cell=(8, 8),\n",
    "                     cells_per_block=(2, 2), visualize=True)\n",
    "    \n",
    "    axes[0, i+2].imshow(hog_pos, cmap='gray')\n",
    "    axes[0, i+2].set_title(f'HOG Pos {i+1}')\n",
    "    axes[0, i+2].axis('off')\n",
    "    \n",
    "    axes[1, i+2].imshow(hog_neg, cmap='gray')\n",
    "    axes[1, i+2].set_title(f'HOG Neg {i+1}')\n",
    "    axes[1, i+2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(images):\n",
    "    \"\"\"Extract HOG features from images\"\"\"\n",
    "    features = []\n",
    "    for img in images:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        hog_feat = hog(gray, orientations=9, pixels_per_cell=(8, 8), \n",
    "                      cells_per_block=(2, 2), visualize=False)\n",
    "        features.append(hog_feat)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract HOG features\n",
    "print(\"Extracting HOG features...\")\n",
    "X_train_hog = extract_hog_features(X_train_imgs)\n",
    "X_val_hog = extract_hog_features(X_val_imgs)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_hog)\n",
    "X_val = scaler.transform(X_val_hog)\n",
    "joblib.dump(scaler, \"Models/scaler.joblib\")\n",
    "\n",
    "\n",
    "print(f\"Feature shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with optimized hyperparameters\n",
    "models = {\n",
    "    'SVM': SVC(\n",
    "        C=1,\n",
    "        kernel='rbf', \n",
    "        gamma='scale', \n",
    "        probability=True, \n",
    "        random_state=42\n",
    "    ),\n",
    "    'Neural Network': MLPClassifier(\n",
    "        hidden_layer_sizes=(100,), \n",
    "        activation='relu', \n",
    "        solver='adam', \n",
    "        alpha=0.001, \n",
    "        learning_rate='constant', \n",
    "        max_iter=1000, \n",
    "        random_state=42\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=500, \n",
    "        max_depth=3, \n",
    "        learning_rate=0.1, \n",
    "        subsample=1.0, \n",
    "        random_state=42\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        C=0.01, \n",
    "        solver='lbfgs', \n",
    "        penalty='l2', \n",
    "        max_iter=2000, \n",
    "        random_state=42\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100, # You can adjust based on your specific RF result\n",
    "        random_state=42\n",
    "    ),\n",
    "    'KNN': KNeighborsClassifier(\n",
    "        n_neighbors=3, \n",
    "        weights='distance', \n",
    "        metric='manhattan'\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Train and Evaluate Models\n",
    "# ====================================================\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training Optimized {name}...\")\n",
    "    \n",
    "    # Train model (using your pre-scaled X_train)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Accuracies\n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train_Acc': train_acc,\n",
    "        'Val_Acc': val_acc,\n",
    "        'Overfit_Gap': train_acc - val_acc\n",
    "    })\n",
    "    \n",
    "    # Save the final optimized model\n",
    "    joblib.dump(model, f\"Models/{name.replace(' ', '_')}_final.joblib\")\n",
    "    \n",
    "    print(f\"{name} Results -> Train: {train_acc:.4f}, Val: {val_acc:.4f}\")\n",
    "\n",
    "# ====================================================\n",
    "# Save Final Comparison\n",
    "# ====================================================\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Val_Acc', ascending=False)\n",
    "results_df.to_csv('Results/final_model_performance.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*30)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_pos = np.arange(len(results_df))\n",
    "\n",
    "plt.bar(x_pos - 0.2, results_df['Train_Acc'], 0.4, label='Train Accuracy', alpha=0.8)\n",
    "plt.bar(x_pos + 0.2, results_df['Val_Acc'], 0.4, label='Validation Accuracy', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x_pos, results_df['Model'], rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('Results/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model analysis\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = joblib.load(f\"Models/{best_model_name.replace(' ', '_')}.joblib\")\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Validation Accuracy: {results_df.iloc[0]['Val_Acc']:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "val_pred = best_model.predict(X_val)\n",
    "cm = confusion_matrix(y_val, val_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Non-Pedestrian', 'Pedestrian'])\n",
    "plt.yticks(tick_marks, ['Non-Pedestrian', 'Pedestrian'])\n",
    "\n",
    "# Add text annotations\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Results/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, val_pred, target_names=['Non-Pedestrian', 'Pedestrian']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Step 10 — Test All Models on HOG_NORMALIZED Test Set\n",
    "# ====================================================\n",
    "\n",
    "# Load test data\n",
    "test_pos = load_pos(\"../INRIAPerson/Test/pos\")\n",
    "test_neg = load_neg(\"../INRIAPerson/Test/neg\")\n",
    "\n",
    "X_test_imgs = test_pos + test_neg\n",
    "y_test = np.array([1]*len(test_pos) + [0]*len(test_neg))\n",
    "\n",
    "print(f\"Test samples: {len(X_test_imgs)}\")\n",
    "\n",
    "# Extract HOG features for test set\n",
    "X_test_hog = extract_hog_features(X_test_imgs)\n",
    "\n",
    "# Load scaler for normalized features\n",
    "scaler = joblib.load(\"Models/scaler.joblib\")  # Save this during training\n",
    "\n",
    "# Normalize test features\n",
    "X_test_norm = scaler.transform(X_test_hog)\n",
    "\n",
    "# List of model names\n",
    "model_names = [\"SVM\", \"Random_Forest\", \"KNN\", \"Logistic_Regression\", \"Gradient_Boosting\", \"Neural_Network\"]\n",
    "\n",
    "# Loop through each model\n",
    "for name in model_names:\n",
    "    print(f\"\\nTesting Model: {name}\")\n",
    "    \n",
    "    # Load trained model\n",
    "    model = joblib.load(f\"Models/{name}.joblib\")\n",
    "    \n",
    "    # Predict\n",
    "    test_pred = model.predict(X_test_norm)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    \n",
    "    print(f\"{name} Test Accuracy: {test_acc:.4f}\")\n",
    "    print(classification_report(y_test, test_pred, target_names=['Negative','Positive']))\n",
    "    \n",
    "    # Find correct and wrong predictions\n",
    "    correct_idx = np.where(test_pred == y_test)[0]\n",
    "    wrong_idx = np.where(test_pred != y_test)[0]\n",
    "    \n",
    "    # Randomly select 6 from each\n",
    "    correct_sample = np.random.choice(correct_idx, min(6, len(correct_idx)), replace=False)\n",
    "    wrong_sample = np.random.choice(wrong_idx, min(6, len(wrong_idx)), replace=False)\n",
    "    \n",
    "    # --- Visualize correct predictions ---\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    axes = axes.ravel()\n",
    "    for i, idx in enumerate(correct_sample):\n",
    "        img = cv2.cvtColor(X_test_imgs[idx], cv2.COLOR_BGR2RGB)\n",
    "        true_label = 'Pedestrian' if y_test[idx] == 1 else 'Non-Pedestrian'\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'True: {true_label}', color='green', fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(f\"{name} — 6 Correct Predictions\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Visualize wrong predictions ---\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    axes = axes.ravel()\n",
    "    for i, idx in enumerate(wrong_sample):\n",
    "        img = cv2.cvtColor(X_test_imgs[idx], cv2.COLOR_BGR2RGB)\n",
    "        true_label = 'Pedestrian' if y_test[idx] == 1 else 'Non-Pedestrian'\n",
    "        pred_label = 'Pedestrian' if test_pred[idx] == 1 else 'Non-Pedestrian'\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}', color='red', fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(f\"{name} — 6 Wrong Predictions\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
